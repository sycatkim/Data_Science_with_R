---
title: "479 Project"
author: "Zhengyun Dou"
date: "4/19/2018"
output: html_document
---

```{r,warning=F,message=F,results='hide',echo=FALSE}
#list of required packages
library(devtools)
library(rvest)
library(tidyverse)
library(stringr)
library(quantmod) #financial package
#library(twitteR) #twitter from R
#library(rtweet)
library(base64enc) #supplementary package to the above
library(lubridate)
library(tidytext)
library(widyr)
#library(igraph)
#library(ggraph)
library(tm)
library(wordcloud)
library(wordcloud2)
library(Matrix)
library(rARPACK)
library(data.table)
library(tidyr)
library(sunburstR)
library(aTSA)
library(TSA)
library(ggplot2)
```

#After Mass Shooting

Based on the records on [MotherJones](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/), from 2017-01-01 to 2018-03-09, there are totally 14 mass shootings (with fatalities 3 or above). 

```{r,warning=F,message=F,echo=F}
mass=read_csv("Mother Jones' Investigation_ US Mass Shootings, 1982-2018 - US mass shootings.csv")
# from https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/
# str(mass)
# head(mass)
mass2017=mass %>% 
  mutate(Date=mdy(mass$Date)) %>% 
  filter(Date>="2017-01-01") #covert the date to be the same format as in stock[]

mass2017[,c(2,3,5,6)]
```


###Our analysis consists of two parts,

+ were the stock prices of the gun manufactuers affected after mass shooting,
+ what does the public talk about on guns.


##1. Stock prices of the gun manufactuers

There are four major gun manuafcturers in the US that go pubnlic, namely,

Company Name  | Stock Symbol
------------- | -------------
 American Outdoor Brands   | AOBC
 Sturm, Ruger     | RGR
 Vista Outdoor   | VSTO
 Olin     | OLN

The stocks of these four companies from 2017-01-01 to 2018-04-23.

```{r,warning=F,message=F,echo=FALSE}
data=tibble(company=c("American Outdoor Brands","Sturm, Ruger","Vista Outdoor","Olin"),
            symbol=c("AOBC","RGR","VSTO","OLN"),
            agency=c("NASDAQ","NYSE","NYSE","NYSE"))

getSymbols(data$symbol,from="2017-01-01",adjust=TRUE) #all the four companies  
stock=Cl(get(data$symbol[1])) #close column of AOBC
for(i in 2:length(data$symbol))
  stock=merge(stock,Cl(get(data$symbol[i]))) #closure stock price for all four companies     
colnames(stock)=data$symbol
# tail(stock)
stockchange=stock%>% diff
#head(stockchange)

plot(as.zoo(stock), screens = 1, lty = 1:3, xlab = "Date", ylab = "stock")
legend("topleft", c("AOBC", "RGR", "VSTO","OLN"), lty = 1:3, cex = 0.5)
```

We selected the stocks of the four companies on the massing shooting date and the following one week of each mass shooting case, to see if there is a big variation on the stock price.

```{r,warning=F,message=F,echo=F}
mdays=function(a,x){
  day=matrix(nrow = x,ncol=length(a))
  for (i in 1:x){
    day[i,]=a+i}
  c(a,day)}
#create a function to extend the mass shooting date to the following x days

daterange1=mdays(mass2017$Date,7)
case=rep(1:14,c(6,6,6,2,5,6,5,6,3,6,5,5,5,6)) %>% as.tibble()

variance=stock[daterange1] %>% as.tibble() %>% 
  mutate(date=index(stock[daterange1])) %>%
  cbind(.,case) %>% rename(shootingcase=value) %>% 
  group_by(shootingcase) %>% 
  summarise(varianceAOBC=var(AOBC),varianceRGR=var(RGR),
            varianceVSTO=var(VSTO),varianceOLN=var(OLN)) %>% 
  mutate(date=sort(mass2017$Date)) %>% select(shootingcase,date,varianceAOBC:varianceOLN)

variance[,1:5]
```

Apart from the varianceds of VSTO in case 1 and case 10, the other variances are comparablly small, so we take a close look at these two cases.


```{r,warning=F,message=F,echo=F}

stock$VSTO[mdays(ymd("2017-01-06"),7)]
stock$VSTO[mdays(ymd("2017-11-05"),7)]
```

Even thought there was big changes on the stock price, but the changes happened after 3 days of the mass shooting, so we didn't see this change is directly related to the mass shooting.

Alternatively, we make a plot to show the stock price with one week after mass shooting to visualize how the prices changed over time.

```{r,warning=F,message=F,echo=F}

stock[daterange1] %>% as.tibble() %>% 
  mutate(date=index(stock[daterange1])) %>%cbind(.,case) %>%  
  gather('AOBC','RGR','VSTO','OLN',key="Co",value="price") %>% 
  ggplot(aes(y=price,x=date))+
  geom_point()+
  facet_grid(Co~value)

#stock%>% diff %>% .[mdays(ymd("2017-01-06"),7)]
#stock%>% diff %>% .[mdays(ymd("2017-11-05"),7)]
```

Similarly, apart from the two cases aforementioned, other prices are all centralized with similar values within that week. So we didn't see there is a change on the stock price after mass shooting.

In order to further analyze this problem, we use volatility analysis to see if there is any impact.

The following part is for the volatility analysis. we build ARIMA model and extract the residuals from it. we regarded the \$residual^2$ as the volatility.

We choose AOBC, RGR, VSTO, OLN top four gun company stock adjusted price as our data. The data are all from 2017-01-03 to 2018-04-18.

For the four company seems like they all have unit root and we need to take difference.

After differentiation the RGR and VSTO are already convert to stationary with no ACF and PACF in series. Thus we treated the diff.adjusted.price^2 as volatility.

As for the rest two series which is AOBC, OLN. After differentiation we fited ARMA model to get the residuals and treated residuals^2 as volatility.

```{r, echo=T, results='hide'}
#AOBC
getSymbols("AOBC") 
#acf(AOBC$AOBC.Adjusted,lag.max = 1000)
#pacf(AOBC$AOBC.Adjusted,lag.max = 1000)
#plot(AOBC$AOBC.Adjusted)
#unit root test
aTSA::adf.test(AOBC$AOBC.Adjusted); tseries::adf.test(AOBC$AOBC.Adjusted)
AOBC.Adjusted.diff<-diff(AOBC$AOBC.Adjusted)[-1]
par(mfrow=c(1,2))
#acf(AOBC.Adjusted.diff)
#pacf(AOBC.Adjusted.diff)
#Thus the original data in random walk
##
#eacf(AOBC.Adjusted.diff,ar.max=10,ma.max=10)
Box.test(AOBC.Adjusted.diff,type="Ljung-Box",lag=12)
#ARIMA(3,0,3)
#ARIMA(3,0,6)
#ARIMA(0,0,4)
m1<-arima(AOBC.Adjusted.diff,order=c(3,0,3),include.mean = TRUE)
m2<-arima(AOBC.Adjusted.diff,order=c(3,0,6),include.mean = TRUE)
m3<-arima(AOBC.Adjusted.diff,order=c(0,0,6),include.mean = TRUE)
#confint(m1);tsdiag(m1)
#confint(m2);tsdiag(m2)
#confint(m3);tsdiag(m3)
Box.test(m1$residuals,lag=12,fitdf = 1,type="Ljung-Box")
Box.test(m2$residuals,lag=12,fitdf = 1,type="Ljung-Box")
Box.test(m3$residuals,lag=12,fitdf = 1,type="Ljung-Box")
m4<-arima(AOBC.Adjusted.diff,order=c(0,0,6),include.mean = TRUE,fixed=c(0,0,0,NA,0,NA,0))
#confint(m4);tsdiag(m4)
Box.test(m4$residuals,lag=12,fitdf = 1,type="Ljung-Box")
resi_square<-m4$residuals^2
volatility1<-xts(x = resi_square,order.by = index(AOBC.Adjusted.diff))
plot(volatility1)
plot(AOBC.Adjusted.diff)
```

```{r, echo=T, results='hide'}
#RGR
getSymbols("RGR") 
#acf(RGR$RGR.Adjusted)
#pacf(RGR$RGR.Adjusted)
#plot(RGR$RGR.Adjusted)
aTSA::adf.test(RGR$RGR.Adjusted); tseries::adf.test(RGR$RGR.Adjusted)
Box.test(RGR$RGR.Adjusted,type="Ljung-Box",lag=12)
RGR.Adjusted.diff<-diff(RGR$RGR.Adjusted)[-1]
par(mfrow=c(1,2))
#acf(RGR.Adjusted.diff)
#pacf(RGR.Adjusted.diff)
#eacf(RGR.Adjusted.diff)
Box.test(RGR.Adjusted.diff,type="Ljung-Box",lag=12)
plot(RGR.Adjusted.diff)
volatility2<-xts(x=(RGR.Adjusted.diff-mean(RGR.Adjusted.diff))^2,order.by = index(RGR.Adjusted.diff))
plot(volatility2)
```

```{r, echo=T, results='hide'}
#VSTO
getSymbols("VSTO") 
VSTO.Adjusted<-VSTO$VSTO.Adjusted[!is.na(VSTO$VSTO.Adjusted)]
#acf(VSTO.Adjusted)
#pacf(VSTO.Adjusted)
#plot(VSTO.Adjusted)
aTSA::adf.test(VSTO.Adjusted); tseries::adf.test(VSTO.Adjusted)
Box.test(VSTO.Adjusted,type="Ljung-Box",lag=12)
VSTO.Adjusted.diff<-diff(VSTO.Adjusted)[-1]
par(mfrow=c(1,2))
#acf(VSTO.Adjusted.diff)
#pacf(VSTO.Adjusted.diff)
eacf(VSTO.Adjusted.diff)
Box.test(VSTO.Adjusted.diff,type="Ljung-Box",lag=12)
plot(VSTO.Adjusted.diff)
volatility3<-xts(x=(VSTO.Adjusted.diff-mean(VSTO.Adjusted.diff))^2,order.by = index(VSTO.Adjusted.diff))
plot(volatility3)
```


```{r, echo=T, results='hide'}
getSymbols("OLN") 
OLN.Adjusted<-OLN$OLN.Adjusted[!is.na(OLN$OLN.Adjusted)]
#acf(OLN.Adjusted)
#pacf(OLN.Adjusted)
aTSA::adf.test(OLN.Adjusted); tseries::adf.test(OLN.Adjusted)
Box.test(OLN.Adjusted,type="Ljung-Box",lag=12)
OLN.Adjusted.diff<-diff(OLN.Adjusted)[-1]
par(mfrow=c(1,2))
#acf(OLN.Adjusted.diff)
#pacf(OLN.Adjusted.diff)
eacf(OLN.Adjusted.diff)
Box.test(OLN.Adjusted.diff,type="Ljung-Box",lag=12)
m1<-arima(OLN.Adjusted.diff,order=c(4,0,4),include.mean = TRUE)#,fixed=c(0,0,0,NA,0,NA,0))
#confint(m1);tsdiag(m1)
m2<-arima(OLN.Adjusted.diff,order=c(4,0,4),include.mean = TRUE,fixed=c(0,NA,NA,0,0,NA,NA,0,0))
#confint(m2);tsdiag(m2)
Box.test(m1$residuals,type="Ljung-Box",lag=12)
Box.test(m2$residuals,type="Ljung-Box",lag=12)
par(mfrow=c(1,3))
#plot(OLN.Adjusted)
plot(OLN.Adjusted.diff)
volatility4<-xts(x=m2$residuals^2,order.by = index(OLN.Adjusted.diff))
plot(volatility4)
```

```{r}
dataset1<-data.frame(volatility=as.vector(volatility1),idx = index(volatility1), label=index(volatility1))
dataset2<-data.frame(volatility=as.vector(volatility2),idx = index(volatility2), label=index(volatility2))
dataset3<-data.frame(volatility=as.vector(volatility3),idx = index(volatility3), label=index(volatility3))
dataset4<-data.frame(volatility=as.vector(volatility4),idx = index(volatility4), label=index(volatility4))

ggplot(dataset1, aes(x= idx, y= volatility, color="pink", label=label, size=3))+
  geom_point(color = "darkblue", size = 2) + 
  geom_text(aes(label=ifelse(volatility>5,as.character(label),'')),hjust=0,vjust=0)+
  ggtitle("AOBC's volatility") +
  xlab("year") + ylab("volatility")

ggplot(dataset2, aes(x= idx, y= volatility, color="pink", label=label, size=3))+
  geom_point(color = "darkblue", size = 2) + 
  geom_text(aes(label=ifelse(volatility>30,as.character(label),'')),hjust=0,vjust=0)+
  ggtitle("RGR's volatility") +
  xlab("year") + ylab("volatility")

ggplot(dataset3, aes(x= idx, y= volatility, color="pink", label=label, size=3))+
  geom_point(color = "darkblue", size = 2) + 
  geom_text(aes(label=ifelse(volatility>10,as.character(label),'')),hjust=0,vjust=0)+
  ggtitle("VSTO's volatility") +
  xlab("year") + ylab("volatility")

ggplot(dataset4, aes(x= idx, y= volatility, color="pink", label=label, size=3))+
  geom_point(color = "darkblue", size = 2) + 
  geom_text(aes(label=ifelse(volatility>5,as.character(label),'')),hjust=0,vjust=0)+
  ggtitle("OLN's volatility") +
  xlab("year") + ylab("volatility")

```
**Thereofore, based on our analysis, we conclude there is no impact on the gun manufacturers stocks after the mass shooting.**

##2. public opinion on guns

Because of Twitter API limit, we can only get tweets for the past 7 days, and due to the downloading speed, in total, we extracted 100,000 tweets pertaining to guns on 2018-04-20.

+ Based on these tweets, after cleaning the text, we analysed the frequent word(s) people discussed the most.

```{r,,warning=F,message=F,echo=F}
twitters=read_csv("gun.tweets.csv",col_types = cols(.default = col_character())) %>% as.tbl()
#dim(twitters)

twitters$text=sapply(twitters$text,function(row) iconv(row, "latin1", "ASCII", sub=""))

cleantext <- function(tx){
  tx= gsub("htt.{1,20}", " ", tx, ignore.case=TRUE)
  tx = gsub("[^#[:^punct:]]|@", " ", tx, perl=TRUE, ignore.case=TRUE)
  tx = gsub("[[:digit:]]", " ", tx, ignore.case=TRUE)
  tx = gsub(" {1,}", " ", tx, ignore.case=TRUE)
  tx = gsub("^\\s+|\\s+$", " ", tx, ignore.case=TRUE)
  return(tx)
}

twitters$text=lapply(twitters$text, cleantext)

stop_words=rbind(stop_words,c("rt",""),c("amp",""))
#twitters$text=str_replace_all(twitters$text,"^suppo$","support")

#head(twitters$text)
cleaned=twitters  %>% 
  select(text) %>% 
  unnest_tokens(word,text) %>% 
  anti_join(stop_words) 


frequency=cleaned %>% 
  count(word,sort=TRUE) 

#frequency$word=str_replace(frequency$word,"^suppo$","support")

frequency %>% filter(n>2500) %>% ggplot(aes(x=word, y=n)) + geom_bar(stat="identity",fill="skyblue") + xlab("Word") + ylab("Count") + coord_flip() + theme(axis.text=element_text(size=7))

top10=frequency%>% top_n(10) 
top10
```

From the above plot, we are able to see the top words with frequency over 2500.

To make it clear, we print the top 10 words.

We use world cloud to display the frequent words.

```{r,,warning=F,message=F,echo=F}

wordcloud2(frequency,figPath = "gunman.jpg",size=2,color="skyblue")

```

Based on the 10 top words, we are wondering how may tweets contains how may of them in what way, so we make a plot to show the number of the top 10 words, and the number of tweets containing them.

```{r,,warning=F,message=F,echo=F}
key=top10$word
key_match=str_c(key, collapse = "|")

plot(sort(str_count(twitters$text, key_match),decreasing=TRUE),type="l",xlab = "Tweets", ylab = "Topwords")

```

With the increase of the top words, the tweets containing all of them reduce. We are interested into those tweets with multiple top words, in this case, we say top words more than 6. 

Below is a display of 10 entries.

```{r,,warning=F,message=F,echo=F}
sentence=twitters$text[str_count(twitters$text, key_match)> 6]

unlist(sentence[1:10])
```

Apart from tweet 10, all the others are all in the support of gun control. 

In addition to the single word, we are also interested in the word associations, e.g. what words occur with gun together mostly. We print out the top 5 word associations.

```{r,warning=F,message=F,echo=F}
cleaned_two=twitters%>% 
  select(text) %>% 
  unnest_tokens(words,text,token="ngrams",n=2,collapse = FALSE) 


separated=cleaned_two %>% 
  separate(words,c("word1","word2"),sep=" ")

filtered=separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1,word2,sort=TRUE)

filtered %>% top_n(5)

```

Gun control and gun violence are the top two.

We also plot this result to show it in a gram with connections between the words.

```{r,,warning=F,message=F,echo=F}
filtered %>% filter(n>400) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout="fr")+
  geom_edge_link(aes(edge_alpha=n,edge_width=n))+
  geom_node_point(color = "skyblue", size = 3) +
  geom_node_text(aes(label = name), vjust = 1.8, size = 3)

```


We use SVD to analyze the tweets. The result shows the tweets are being tweeted mostly, and the fist engien vector is the tweet from Ted Cruz, which is being retweeted the most.


```{r,warning=F,message=F,echo=F}

textdf=data_frame(tweet=1:nrow(twitters),text=unlist(twitters$text))

tword=textdf %>% unnest_tokens(word,text) 

dt = cast_sparse(tword,tweet,word)
#str(dt)
# dim(dt)
# hist(rowSums(dt))
# cs = colSums(dt)
# hist(log(cs[cs>1]))
# dt=cast_sparse(tt,1:nrow(twitters),word)

A = dt; Dl = Diagonal(nrow(A), 1/sqrt(rowSums(A)+10)); Dr = Diagonal(ncol(A), 1/sqrt(colSums(A)+10))
L = Dl%*%A%*%Dr
s = svds(L, k = 10)
#plot(s$d[-1])
u = s$u 

#plot(as.data.frame(u[sample(nrow(A),1000),]), pch = ".")
textdf[which(s$u[,2]< -.015),]$text[1]
```


```{r,warning=F,message=F,echo=F,results="hide"}
retweet=unlist(twitters$text[duplicated(twitters$text)])
retweet %>% as.tibble() %>%count(value,sort =TRUE) %>% .[2:10,] %>% 
  rename(tweet=value) %>% 
 select(tweet)

```